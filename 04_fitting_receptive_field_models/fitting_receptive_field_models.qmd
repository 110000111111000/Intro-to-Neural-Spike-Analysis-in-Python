---
title: "Fitting Receptive Field Models"
engine: Jupyter
format: ipynb
filters:
    - assign
execute: 
  cache: true
number-sections: true
number-depth: 2
---

## Preparation

```{python}
import numpy as np
import matplotlib.pyplot as plt
from matplotlib.patches import Ellipse
from scipy.optimize import curve_fit
from scipy import stats
```

```{python}
class utils:
    def exponential(x, a, b, c):
        return a * np.exp(b * x) + c

    def linear(x, a, b):
        return a + x * b

    def polynomial(x, a, b, c, d):
        return a + b * x + c * x**2 + d * x**3

    def gaussian_2d(xy, b, x0, y0, s_x, s_y, a):
        x, y = xy
        gaussian = (
            b * np.exp(-((x - x0) ** 2 / (2 * s_x**2) + (y - y0) ** 2 / (2 * s_y**2)))
            + a
        )
        return gaussian

    def generate_data1():
        x = np.linspace(0, 4, 100)
        y = utils.exponential(x, 2, 0.8, 1)
        noise_level = 0.3 * np.max(y)
        noise = np.random.normal(0, noise_level * np.exp(-0.2 * x), len(x))
        y += noise
        return x, y

    def generate_data2():
        x_range = np.linspace(-3, 3, 50)
        y_range = np.linspace(-2, 4, 50)
        X, Y = np.meshgrid(x_range, y_range)
        x = X.flatten()
        y = Y.flatten()
        z = utils.gaussian_2d((x, y), 8.0, 0.5, 1.0, 1.2, 0.8, 2.0)
        noise_level = 0.2 * 8.0
        noise = np.random.normal(0, noise_level, len(z))
        z += noise
        z = z.reshape(X.shape)
        return x, y, z

    def confidence_intervals(n_data, params, pcov, ci=0.95):
        alpha = 1 - ci
        dof = n_data - len(params)
        t_val = stats.t.ppf(1 - alpha / 2, dof)
        param_errors = np.sqrt(np.diag(pcov))
        ci_lower = params - t_val * param_errors
        ci_upper = params + t_val * param_errors
        return ci_lower, ci_upper

    def r_squared(y, y_fit):
        ss_res = np.sum((y - y_fit) ** 2)
        ss_tot = np.sum((y - np.mean(y_fit)) ** 2)
        r2 = 1 - (ss_res / ss_tot)
        return r2
```

## Fitting Curves

:::{#exm-}
Generate data and visualize it using a `scatter` plot.
:::
```{python}
x, y = utils.generate_data1()
plt.scatter(x, y)
```

:::{#exm-}
Use `curve_fit` fit the `utils.linear` function to the points `x`, `y`and return the optimized parameters `popt` and their covaraince `pcov`.
:::
```{python}
popt, pcov = curve_fit(utils.linear, x, y)
```

:::{#exm-}
Use the `utils.linear` function with the optimized parameters `popt` to generate the values `y_fit` and plot them together with the original data points.
:::
```{python}
y_fit = utils.linear(x, popt[0], popt[1])
plt.scatter(x, y)
plt.plot(x, y_fit, linestyle='--', color='black', label="best fit")
plt.legend()
```

:::{#exr-}
Fit the data points `x`, `y` using the `utils.exponential` function and return the optimized parameters `popt` and their covariance `pcov`.
:::
:::{.sol}
```{python}
popt, pcov = curve_fit(utils.exponential, x, y)
```
:::

:::{#exr-}
Use the `utils.exponential` function with optimized parameters `popt` to generate the values `y_fit` an plot them together with the original data points (HINT: the exponential functin takes 3 parameters).
:::
:::{.sol}
```{python}
y_fit = utils.exponential(x, popt[0], popt[1], popt[2])
plt.scatter(x, y)
plt.plot(x, y_fit, linestyle='--', color='black', label="best fit")
plt.legend()
```
:::

:::{#exr-}
Fit the data points `x`, `y` using the `utils.polynomial` function and return the optimized parameters `popt` and their covariance `pcov`.
:::
:::{.sol}
```{python}
popt, pcov = curve_fit(utils.polynomial, x, y)
```
:::


:::{#exr-}
Use the `utils.polynomial` function with optimized parameters `popt` to generate the values `y_fit` an plot them together with the original data points (HINT: insetad of passing the elements of `popt` individually, you can pass them as a list `*popt`).
:::
:::{.sol}
```{python}
y_fit = utils.polynomial(x, *popt)
plt.scatter(x, y)
plt.plot(x, y_fit, linestyle="--", color="black", label="best fit")
plt.legend()
```
:::

## Determining the Goodness of Fit

:::{#exm-}
Fit the `utils.linear` function to the data `x`, `y` and use the optimized parameters to generate `y_fit`. Then, compute the ratio of variance explained by the model using the function `utils.r_squared()`.
:::
```{python}
popt, pcov = curve_fit(utils.linear, x, y)
y_fit = utils.linear(x, *popt)
utils.r_squared(y, y_fit)
```

:::{#exr-}
Compute `utils.r_squared` for the fit of the `utils.exponential` function. Does this model fit the data better or worse than the linera function accoring to R^2?
:::
:::{.sol}
```{python}
popt, pcov = curve_fit(utils.exponential, x, y)
y_fit = utils.exponential(x, *popt)
utils.r_squared(y, y_fit)
```
:::

:::{#exr-}
Compute `utils.r_squared` for the fit of the `utils.polynomial` function. Does this model fit the data better or worse than the linear function according to R^2?
:::
```{python}
popt, pcov = curve_fit(utils.polynomial, x, y)
y_fit = utils.polynomial(x, *popt)
utils.r_squared(y, y_fit)
```

:::{#exm-}
Compute the standard error for the optimized parameters from the `utils.exponential` curve fit by taking the square root of the diagonal of the parameter covariance matrix `pcov`. The, divide the `param_error` by the absolute value of the parameters and multiply by `100` to get the uncertainty in percent.
:::
```{python}
popt, pcov = curve_fit(utils.exponential, x, y)
param_errors = np.sqrt(np.diag(pcov))
param_errors / np.abs(popt) * 100
```

:::{#exr-}
Compute the uncertainty in percent for the optimized parameters from the `utils.linear` curve fit. For which parameter is the uncertainty higher, the intercept (a) or slope (b)?
:::
:::{.sol}
```{python}
popt, pcov = curve_fit(utils.linear, x, y)
param_errors = np.sqrt(np.diag(pcov))
param_errors / np.abs(popt) * 100
```
:::

:::{#exr-}
Compute the uncertainty in percent for the optimized parameters from the `utils.polynomial` curve fit and compare the level of uncertainty from this model to the others.
:::
:::{.sol}
```{python}
popt, pcov = curve_fit(utils.polynomial, x, y)
param_errors = np.sqrt(np.diag(pcov))
param_errors / np.abs(popt) * 100
```
::

:::{#exm-}
Use the function `utils.confidence_intervals()` to 
::::

```{python}
popt, pcov = curve_fit(utils.linear, x, y)
y_fit = utils.linear(x, *popt)

ci_low, ci_high = utils.confidence_intervals(
    n_data=len(x), params=popt, pcov=pcov, ci=0.95
)
y_ci_low = utils.linear(x, *ci_low)
y_ci_high = utils.linear(x, *ci_high)
```

```{python}
plt.scatter(x, y)
plt.plot(x, y_fit, color="black")
plt.fill_between(x, y1=y_ci_low, y2=y_ci_high, color="gray", alpha=0.5)

```

## Fitting a 2-dimensional Gaussian

```{python}
x, y, Z = utils.generate_data2()
plt.imshow(Z)
```

```{python}
popt, pcov = curve_fit(utils.gaussian_2d, (x, y), Z.flatten())
```

```{python}
z_fit = utils.gaussian_2d((x, y), *popt)
Z_fit = z_fit.reshape(Z.shape)
plt.subplot(1, 2, 1)
plt.imshow(Z)
plt.subplot(1, 2, 2)
plt.imshow(Z_fit)
```

```{python}
x0, y0, sigma_x, sigma_y = popt[1:5]
theta = np.linspace(0, 2 * np.pi, 100)
x_ellipse = x0 + sigma_x * np.cos(theta)
y_ellipse = y0 + sigma_y * np.sin(theta)
plt.imshow(Z, extent=[-3, 3, -2, 4])
plt.plot(x_ellipse, y_ellipse, color="black")
```

```{python}
x0, y0, sigma_x, sigma_y = popt[1:5]
theta = np.linspace(0, 2 * np.pi, 100)
x_ellipse = x0 + 2*sigma_x * np.cos(theta)
y_ellipse = y0 + 2*sigma_y * np.sin(theta)
plt.imshow(Z, extent=[-3, 3, -2, 4])
plt.plot(x_ellipse, y_ellipse, color="black")
```

## Extending the Model