
```{python}
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
```

## Preparation

Download the data required for this session
```{python}
import requests

url = "https://uni-bonn.sciebo.de/s/aFQ1gcUbOHRDEtP"
fname = "spike_waveforms.npy"
response = requests.get(f"{url}/download")
print("Downloading Data ...")
with open(fname, "wb") as file:
    file.write(response.content)
print("Done!")
```

## Extraing Waveform Features with PCA

| Code | Description |
| --- | --- |
| `x = np.load("data.npy")` | Load the file `"data.npy"` and assign its content to the variable `x` |
| `pca=PCA(n_components=n)` | Create an instance of the `PCA` object with `n` components |
| `X_transformed=pca.fit_transform(X)` | Fit the `pca` to the matrix `X`, transform the data and assign the result to a new variable `X_transformed` |
| `pca.explained_variance_ratio_` | Get the ratio of variance explained by each principle component in the fitted `pca` |
| `np.cumsum(pca.explained_variance_ratio_)` | Calculate the cumulative sum of the ratio of variance explained by the principle components |

---

:::{#exr-}
Load the array of of spike waveforms stored in `"../data/spike_waveforms.npy"`, assign it to a variable `waveforms` print its `.shape`. How many spikes are stored in the array?
:::
:::{.sol}
```{python}
waveforms = np.load("../data/spike_waveforms.npy")
waveforms.shape
```
:::

:::{#exm-}
Use `PCA` with `n_components=5` to transform the spike waveforms recorded at the first channel.
:::
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=5)
X_transformed = pca.fit_transform(X)
```

:::{#exr-}
Use `PCA` with `n_components=7` to transform the spike waveforms recorded at the first channel. Compare the `.shape` of the original and the transformed data.
:::
:::{.sol}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=7)
X_transformed = pca.fit_transform(X)
print(X.shape, X_transformed.shape)
```
:::

:::{#exr-}
Use `PCA` with `n_components=15` to transform the spike waveforms recorded at the first channel. Compare the `.shape` of the original and the transformed data.
:::
:::{.sol}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=15)
X_transformed = pca.fit_transform(X)
print(X.shape, X_transformed.shape)
```
:::

:::{#exr-}
What error message do you observe when you try to apply `PCA` with `n_components=40`?
:::
:::{.sol}
`ValueError: n_components=40 must be between 0 and min(n_samples,n_features)=30`
:::

:::{#exr-}
Plot the ratio of variance explained by the PCA components (i.e. `pca.explained_variance_ratio_`)
:::
:::{.sol}
```{python}
plt.plot(pca.explained_variance_ratio_)
```
:::


:::{#exr-}
Compute the cumulative ratio of explained variance using `np.cumsum()` and plot it
:::
:::{.sol}
```{python}
plt.plot(np.cumsum(pca.explained_variance_ratio_))
```
:::

:::{#exr-}
What ratio of the variance is explained by the first three components (i.e. what is the `sum()` of the first three element of `.explained_varaince_ratio_`?) and how many components are required to account for over 99% of varaince?
:::
:::{.sol}
```{python}
print(sum(pca.explained_variance_ratio_[:3]))
print(sum(pca.explained_variance_ratio_[:8]))
```
:::

:::{#exr-}
Fit a PCA with `n_components=15` to the waveforms recorded at the second channel, then compute and plot the PCA component's cumulative ratio of explained variance.
:::
:::{.sol}
```{python}
X = waveforms[:, :, 1]
pca = PCA(n_components=15)
X_transformed = pca.fit_transform(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
```
:::

## Visualizing Principle Components

| Code | Description |
| --- | --- |
| `pca.components_` | Get the components of the fitted `pca` |
| `X_inverse = pca.inverse_transform(X_transformed)`| Apply the inverse `pca` transformation to `X_transformed` and assign the result to a new variable `X_inverse` |

---

:::{#exm-}
Fit a `PCA` with `n_components=3` to the waveforms recorded at the first channel and plot the first component, along with the first 100 recorded waveforms.
:::
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=3)
X_transformed = pca.fit_transform(X)
plt.subplot(2, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(2, 1, 2)
plt.plot(pca.components_[0])
```

:::{#exr-}
Plot the second and third principle component along with the first 100 waveforms recorded at the first channel.
:::
:::{.sol}
```{python}
plt.subplot(3, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(3, 1, 2)
plt.plot(pca.components_[1])
plt.subplot(3, 1, 3)
plt.plot(pca.components_[2])
```
:::

:::{#exr-}
Fit a `PCA` with `n_components=3` to the waveforms recorded at the third channel and plot the first component, along with the first 100 recorded waveforms.
:::
:::{.sol}
```{python}
X = waveforms[:, :, 2]
pca = PCA(n_components=3)
X_transformed = pca.fit_transform(X)
plt.subplot(2, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(2, 1, 2)
plt.plot(pca.components_[0])
```
:::

:::{#exm-}
Compute a `PCA` with `n_components=1` for the waveforms recorded at the first channel and transform the data. Then, compute the `.inverse_transfom()` of the transformed data and plot the result of the inverse transformation `X_inverse` along with the original data `X` for the first spike.
:::
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=1)
X_transformed = pca.fit_transform(X)
X_inverse = pca.inverse_transform(X_transformed)
plt.plot(X[0])
plt.plot(X_inverse[0])
```

:::{#exr-}
Plot the original data `X` and the result of the inverse transformation `X_inverse` for the 7th spike.
:::
:::{.sol}
```{python}
plt.plot(X[6])
plt.plot(X_inverse[6])
```
:::

:::{#exr-}
Re-compute the `PCA` and increase the number of `n_components`, then `fit_transform` and `inverse_transform` the data. How many components do you need so the original data `X` and `X_inverse` become indistinguishable?
:::
:::{.sol}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=9)
X_transformed = pca.fit_transform(X)
X_inverse = pca.inverse_transform(X_transformed)
plt.plot(X[6])
plt.plot(X_inverse[6])
```
:::

:::{#exr-}
Complete the `for` loop below by computing `X_transformed` for the spike waveforms `X` at each channel `i`.
:::
```{python}
#| eval: false
n_components = 3
pca = PCA(n_components=n_components)
n_spikes = waveforms.shape[0]
n_channels = waveforms,shape[-1]
waveforms_transformed = np.zeros((n_spikes, n_components, n_channels))

for i_ch in range(n_channels):
    X = waveforms[:, :, i]
    X_transformed = # your code here 
    waveforms_transformed[:, :, i_ch] = X_transformed
```
:::{.sol}
```{python}
n_components = 3
pca = PCA(n_components=n_components)
n_spikes = waveforms.shape[0]
n_channels = waveforms.shape[-1]
waveforms_transformed = np.zeros((n_spikes, n_components, n_channels))
for i in range(n_channels):
    X = waveforms[:, :, i]
    X_transformed = pca.fit_transform(X)
    waveforms_transformed[:, :, i] = X_transformed
```
:::

:::{#exm-pca}
Plot the amplitude of the 1st PCA feature at the channel 1 against the amplitude at the channel 2 for all spikes (each spike is one dot).
:::
```{python}
plt.scatter(waveforms_transformed[:, 0, 0], waveforms_transformed[:, 0, 1], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```

:::{#exr-}
Plot the amplitude of the 1st PCA feature at channel 1 against the amplitude at channel 3 for all spikes. Can you visually identify clusters that the individual spikes may fall into?
:::
:::{.sol}
```{python}
plt.scatter(waveforms_transformed[:, 0, 0], waveforms_transformed[:, 0, 2], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```
:::

:::{#exr-}
Plot the amplitude of the 1st PCA feature at channel 3 against the amplitude at channel 4 for all spikes. Can you visually identify clusters that the individual spikes may fall into?
:::
:::{.sol}
```{python}
plt.scatter(waveforms_transformed[:, 0, 2], waveforms_transformed[:, 0, 3], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```
:::

## Clustering Spikes with Gaussian Mixture Models

| Code | Description |
| --- | --- |
| `gmm=GaussianMixture(n_components)` | Create a `GaussianMixture` model with `n` components |
| `gmm.fit(X)` | Fit the gaussian mixture model `gmm` to the data matrix `X` |
| `gmm.score(X)` | Compute the `score` (i.e. the log-likelihood) of the fitted model `gmm` on the data `X` |
| `gmm.bic(X)` | Compute the Baysian Information Criterion `bic` of the fitted model `gmm` on the data `X` |

:::{#exr-}
Execute the cell below to `.reshape()` the 3-dimensional matrix `waveforms_transformed` into a 2-dimensional matrix `X` by concatenating the components and channels dimension. What is the new `.shape` of `X`?
:::
```{python}
X = waveforms_transformed.reshape(-1, n_components * n_channels, order="F")
```
:::{.sol}
```{python}
X.shape
```
:::

:::{#exm-}
Apply a `GaussianMixture` model with `n_components=5` to the first `1000` spikes in `waveforms` and compute the models `.score()` (i.e. the log likelihood).
:::
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=10` to the first `1000` spikes in `waveforms` and compute the models `.score()`. Is this model more accurate than the model with `n_components=5`?
:::
:::{.sol}
```{python}
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```
:::


:::{#exr-}
Apply a `GaussianMixture` model with `n_components=100` to the first `1000` spikes in `waveforms` and compute the models `.score()`. Will the model's accuracy always increase with the number of components? What is the largest number of components possible for this model?
:::
:::{.sol}
```{python}
gmm = GaussianMixture(n_components=100)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```
The largest number of components is equal to the number of data points, i.e. 1000
:::

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=10` to the first `1000` spikes in `waveforms` and compute the Bayesian Information Criterion `.bic()`.
:::
:::{.sol}
```{python}
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:1000])
gmm.bic(X[:1000])
```
:::

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=5` to the first `1000` spikes in `waveforms` and compute the Bayesian Information Criterion `.bic()`. Does this model perform better or worse than the one with `n_components=10` according to the BIC (HINT: a lower BIC indicates a better model)?
:::
:::{.sol}
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:1000])
gmm.bic(X[:1000])
```
:::

:::{#exr-}
Increase the number of spikes the `GaussianMixture` model is fit on to `8000`. Which model performs better now, the one with 5 or 10 components?
:::
:::{.sol}
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:8000])
print(gmm.bic(X[:8000]))
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:8000])
print(gmm.bic(X[:8000]))
```
:::

:::{#exr-}
Fit a `GaussianMixture` model with `n_components=13` (which is the optimal number of components according to BIC) to the whole data.
:::
:::{.sol}
```{python}
gmm = GaussianMixture(
    n_components=13,
)
gmm.fit(X)
```
:::

## Predicting and Inspecting Cluster Labels

| Code | Description |
| --- | --- |
| `labels=gmm.predict(X)` | Assign a cluster label to each observation in `X` using the fitted model `gmm` |
| `cluster_probs = gmm.predict_proba(X)` | For each observation in `X` get the probability that it belongs to any cluster in the model `gmm` |

---

:::{#exr-}
Use `gmm.predict` to predict the `spike_labels` for the data `X` and print them.
:::
:::{.sol}
```{python}
spike_labels = gmm.predict(X)
spike_labels
```
:::

:::{#exr-}
How many unique `spike_labels` are there?
:::
:::{.sol}
```{python}
print(len(np.unique(spike_labels)))
```
:::

:::{#exr-}
Use `gmm.predict_proba` to predict the `cluster_probs` for `X` (i.e. the probabilities that each element in `X` belongs to a given cluster) and print their shape.
:::
:::{.sol}
```{python}
cluster_probs = gmm.predict_proba(X)
cluster_probs.shape
```
:::

:::{#exr-}
What is the `spike_label` assigned to the first spike? How many spikes have been assigned to the same label?
:::
:::{.sol}
```{python}
print(spike_labels[0])
sum(spike_labels==spike_labels[0])
```
:::

:::{#exr-}
Plot the probability of cluster membership `cluster_prob` for the first spike as a histogram.
:::
:::{.sol}
```{python}
plt.hist(cluster_probs[0])
```
:::

:::{#exr-}
What is the sum of all `cluster_probs` for a the first spike?
:::
:::{.sol}
```{python}
sum(cluster_probs[0])
```
:::

:::{#exr-}
Get the largest cluster membership probability for each spike (i.e. `cluster_probs.max(axis=1)` and plot them in a histogram.
:::
:::{.sol}
```{python}
plt.hist(cluster_probs.max(axis=1))
```
:::

:::{#exm-}
PLot the amplitude of PCA feature at channel 1 (`X[:, 0, 0]`) against the amplitude at channel 2 (`X[:, 0, 1]`) for the spikes from `clusters=[0, 1, 2, 3]`
:::
```{python}
clusters = [0, 1, 2, 3]
X = waveforms_transformed
plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, color="gray")
for cluster in clusters:
    X = waveforms_transformed[spike_labels == cluster]
    plt.scatter(X[:, 0, 0], X[:, 0, 1], s=5)
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
```

:::{#exr-}
PLot the amplitude of PCA feature at channel 1 (`X[:, 0, 0]`) against the amplitude at channel 3 (`X[:, 0, 2]`) for the spikes from `clusters=[0, 1, 2, 3]`.
:::
:::{.sol}
```{python}
clusters = [0, 1, 2, 3]
X = waveforms_transformed
plt.scatter(X[:, 0, 1], X[:, 0, 3], s=1, color="gray")
for cluster in clusters:
    X = waveforms_transformed[spike_labels == cluster]
    plt.scatter(X[:, 0, 1], X[:, 0, 2], s=1, label=cluster)
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
plt.legend()
```
:::