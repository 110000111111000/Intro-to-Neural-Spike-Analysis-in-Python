
```{python}
import numpy as np
import pandas as pd
from matplotlib import pyplot as plt
from sklearn.decomposition import PCA
from sklearn.mixture import GaussianMixture
```

## Preparation

Download the data required for this session
```{python}
import requests

url = "https://uni-bonn.sciebo.de/s/aFQ1gcUbOHRDEtP/download"
fname = "spike_waveforms.npy"

if not os.path.exists(f"../data/{fname}"):
    response = requests.get(url)
    with open(f"../data/{fname}", "wb") as file:
        file.write(response.content)
```

## Extraing Waveform Features with PCA

| Code | Description |
| --- | --- |
| `x = np.load("data.npy")` | Load the file `"data.npy"` and assign its content to the variable `x` |
| `pca=PCA(n_components=n)` | Create an instance of the `PCA` object with `n` components |
| `X_transformed=pca.fit_transform(X)` | Fit the `pca` to the matrix `X`, transform the data and assign the result to a new variable `X_transformed` |
| `pca.explained_variance_ratio_` | Get the ratio of variance explained by each principle component in the fitted `pca` |
| `np.cumsum(pca.explained_variance_ratio_)` | Calculate the cumulative sum of the ratio of variance explained by the principle components |

---

:::{#exr-}
Load the array of of spike waveforms stored in `"../data/spike_waveforms.npy"`, assign it to a variable `waveforms` print its `.shape`. How many spikes are stored in the array?
:::
:::{sol.}
```{python}
waveforms = np.load("../data/spike_waveforms.npy")
waveforms.shape
```
:::

:::{#exm-}
Use `PCA` with `n_components=5` to transform the spike waveforms recorded at the first channel.
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=5)
X_transformed = pca.fit_transform(X)
```
:::

:::{#exr-}
Use `PCA` with `n_components=7` to transform the spike waveforms recorded at the first channel. Compare the `.shape` of the original and the transformed data.
:::
:::{sol.}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=7)
X_transformed = pca.fit_transform(X)
print(X.shape, X_transformed.shape)
```
:::

:::{#exr-}
Use `PCA` with `n_components=15` to transform the spike waveforms recorded at the first channel. Compare the `.shape` of the original and the transformed data.
:::
:::{sol.}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=15)
X_transformed = pca.fit_transform(X)
print(X.shape, X_transformed.shape)
```
:::

:::{#exr-}
What error message do you observe when you try to apply `PCA` with `n_components=40`?
:::
:::{sol.}
`ValueError: n_components=40 must be between 0 and min(n_samples,n_features)=30`
:::

:::{#exr-}
Plot the ratio of variance explained by the PCA components (i.e. `pca.explained_variance_ratio_`)
:::
:::{sol.}

```{python}
plt.plot(pca.explained_variance_ratio_)
```
:::


:::{#exr-}
Compute the cumulative ratio of explained variance using `np.cumsum()` and plot it
:::
:::{sol.}
```{python}
plt.plot(np.cumsum(pca.explained_variance_ratio_))
```
:::

:::{#exr-}
What ratio of the variance is explained by the first three components (i.e. what is the `sum()` of the first three element of `.explained_varaince_ratio_`?) and how many components are required to account for over 99% of varaince?
:::
:::{sol.}
```{python}
print(sum(pca.explained_variance_ratio_[:3]))
print(sum(pca.explained_variance_ratio_[:8]))
```
:::

:::{#exr-}
Fit a PCA with `n_components=15` to the waveforms recorded at the second channel, then compute and plot the PCA component's cumulative ratio of explained variance.
:::
:::{sol.}
```{python}
X = waveforms[:, :, 1]
pca = PCA(n_components=15)
X_transformed = pca.fit_transform(X)
plt.plot(np.cumsum(pca.explained_variance_ratio_))
```
:::

## Visualizing Principle Components

| Code | Description |
| --- | --- |
| `pca.components_` | Get the components of the fitted `pca` |
| `X_inverse = pca.inverse_transform(X_transformed)`| Apply the inverse `pca` transformation to `X_transformed` and assign the result to a new variable `X_inverse` |

---

:::{#exm-}
Compute a `PCA` with `n_components=3` for the waveforms recorded at the first channel and plot the first component, along with the first 100 recorded waveforms.
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=3)
X_transformed = pca.fit_transform(X)
plt.subplot(2, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(2, 1, 2)
plt.plot(pca.components_[0])
```
:::

:::{#exr-}
Plot the second and third principle component along with the first 100 waveforms recorded at the first channel.
:::
:::{sol.}
```{python}
plt.subplot(3, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(3, 1, 2)
plt.plot(pca.components_[1])
plt.subplot(3, 1, 3)
plt.plot(pca.components_[2])
```
:::

:::{#exr-}
Compute a `PCA` with `n_components=3` for the waveforms recorded at the third channel and plot the first component, along with the first 100 recorded waveforms.
:::
:::{sol.}
```{python}
X = waveforms[:, :, 2]
pca = PCA(n_components=3)
X_transformed = pca.fit_transform(X)
plt.subplot(2, 1, 1)
plt.plot(X[:100].T, linewidth=0.5)
plt.subplot(2, 1, 2)
plt.plot(pca.components_[0])
```
:::

:::{#exm-}
Compute a `PCA` with `n_components=1` for the waveforms recorded at the first channel and transform the data. Then, compute the `.inverse_transfom()` of the transformed data and plot the result of the inverse transformation `X_inverse` along with the original data `X` for the first spike.
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=1)
X_transformed = pca.fit_transform(X)
X_inverse = pca.inverse_transform(X_transformed)
plt.plot(X[0])
plt.plot(X_inverse[0])
```
:::

:::{#exr-}
Plot the original data `X` and the result of the inverse transformation `X_inverse` for the 7th spike.
:::
:::{sol.}
```{python}
plt.plot(X[6])
plt.plot(X_inverse[6])
```
:::

:::{#exr-}
Compute a `PCA` with `n_components=3` for the waveforms recorded at the first channel and transform the data. Then, compute the `.inverse_transfom()` of the transformed data and plot the result of the inverse transformation `X_inverse` along with the original data `X` for the first spike.
:::
:::{sol.}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=3)
X_transformed = pca.fit_transform(X)
X_inverse = pca.inverse_transform(X_transformed)
plt.plot(X[0])
plt.plot(X_inverse[0])
```
:::

:::{#exr-}
How many `PCA` components are required so that the original waveform and the result of the inverse transformation become visually indistinguishable?
:::
:::{sol.}
```{python}
X = waveforms[:, :, 0]
pca = PCA(n_components=9)
X_transformed = pca.fit_transform(X)
X_inverse = pca.inverse_transform(X_transformed)
plt.plot(X[0])
plt.plot(X_inverse[0])
```
:::

:::{#exr-}
Complete the `for` loop below by computing `X_transformed` for the spike waveforms `X` at each channel `i`.
:::
```{python}
#| eval: false
n_components = 3
pca = PCA(n_components=n_components)
n_spikes = waveforms.shape[0]
n_channels = waveforms,shape[-1]
waveforms_transformed = np.zeros((n_spikes, n_components, n_channels))

for i_ch in range(n_channels):
    X = waveforms[:, :, i]
    X_transformed =  
    waveforms_transformed[:, :, i_ch] = X_transformed
```
:::{sol.}
```{python}
n_components = 3
pca = PCA(n_components=n_components)
n_spikes = waveforms.shape[0]
n_channels = waveforms.shape[-1]
waveforms_transformed = np.zeros((n_spikes, n_components, n_channels))
for i in range(n_channels):
    X = waveforms[:, :, i]
    X_transformed = pca.fit_transform(X)
    waveforms_transformed[:, :, i] = X_transformed
```
:::

:::{#exm-pca}
Plot the amplitude of the 1st PCA feature at **channel 1** against the amplitude at **channel 2** for all spikes.
```{python}
plt.scatter(waveforms_transformed[:, 0, 0], waveforms_transformed[:, 0, 1], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```
:::

:::{#exr-}
Plot the amplitude of the 1st PCA feature at **channel 1** against the amplitude at **channel 3** for all spikes. Can you visually identify clusters that the individual spikes may fall into?
:::
:::{sol.}
```{python}
plt.scatter(waveforms_transformed[:, 0, 0], waveforms_transformed[:, 0, 2], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```
:::

:::{#exr-}
Plot the amplitude of the 1st PCA feature at **channel 3** against the amplitude at **channel 4** for all spikes. Can you visually identify clusters that the individual spikes may fall into?
:::
:::{sol.}
```{python}
plt.scatter(waveforms_transformed[:, 0, 2], waveforms_transformed[:, 0, 3], s=1)
plt.xlabel("Ch 1 Amplitude [a.u.]")
plt.ylabel("Ch 2 Amplitude [a.u.]")
```
:::

## Clustering Spikes with Gaussian Mixture Models

| Code | Description |
| --- | --- |
| `gmm=GaussianMixture(n_components)` | Create a `GaussianMixture` model with `n` components |
| `gmm.fit(X)` | Fit the gaussian mixture model `gmm` to the data matrix `X` |
| `gmm.score(X)` | Compute the `score` (i.e. the log-likelihood) of the fitted model `gmm` on the data `X` |
| `gmm.bic(X)` | Compute the Baysian Information Criterion `bic` of the fitted model `gmm` on the data `X` |

:::{#exr-}
Execute the cell below to `.reshape()` the 3-dimensional matrix `waveforms_transformed` into a 2-dimensional matrix `X` by concatenating the components and channels dimension. What is the `.shape` of `X`?
```{python}
X = waveforms_transformed.reshape(-1, n_components * n_channels, order="F")
```
:::
:::{sol.}
```{python}
X.shape
```
:::

:::{#exm-}
Apply a `GaussianMixture` model with `n_components=5` to the first **1000** spikes in `waveforms` and compute the models `.score()` (i.e. the log likelihood).
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```
:::

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=10` to the first **1000** spikes in `waveforms` and compute the models `.score()`. Is this model more accurate than the model with `n_components=5`?
:::
:::{sol.}
```{python}
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```
:::


:::{#exr-}
Apply a `GaussianMixture` model with `n_components=100` to the first **1000** spikes in `waveforms` and compute the models `.score()`. Will the model's accuracy always increase with the number of components? What is the largest number of components possible for this model?
:::
:::{sol.}
```{python}
gmm = GaussianMixture(n_components=100)
gmm.fit(X[:1000])
gmm.score(X[:1000])
```
The largest number of components is equal to the number of data points, i.e. 1000
:::

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=10` to the first **1000** spikes in `waveforms` and compute the Bayesian Information Criterion `.bic()`.
:::
:::{sol.}
```{python}
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:1000])
gmm.bic(X[:1000])
```
:::

:::{#exr-}
Apply a `GaussianMixture` model with `n_components=5` to the first **1000** spikes in `waveforms` and compute the Bayesian Information Criterion `.bic()`. Does this model perform better or worse than the one with `n_components=10` according to the BIC (HINT: a lower BIC indicates a better model)?
:::
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:1000])
gmm.bic(X[:1000])
```
:::

:::{#exr-}
Increase the number of spikes the `GaussianMixture` model is fit on to **8000**. Which model performs better now, the one with 5 or 10 components?
:::
:::{sol.}
```{python}
gmm = GaussianMixture(n_components=5)
gmm.fit(X[:8000])
print(gmm.bic(X[:8000]))
gmm = GaussianMixture(n_components=10)
gmm.fit(X[:8000])
print(gmm.bic(X[:8000]))
```
:::


:::{#exr-}
Complete the for loop below by creating the `GaussianMixture` model for every `n_components` in the `n_components_range`. Then plot the list of `bic` scores against the `n_components_range` and identify where number of components that yield the smalles BIC (NOTE: If the code runs to slowly, you may reduce the number of spikes the models are fit on).
```{python}
n_components_range = range(2, 10 + 1)
bic = []
for n_components in n_components_range:
    print(f"Running GMM with {n_components} components")
    gmm = 
    gmm.fit(X[:3000])
    bic.append(gmm.bic(X[:3000]))
```
:::

:::{sol.}
```{python}
n_components_range = range(2, 10 + 1)
bic = []
for n_components in n_components_range:
    print(f"Running GMM with {n_components} components")
    gmm = GaussianMixture(n_components=n_components)
    gmm.fit(X[:3000])
    bic.append(gmm.bic(X[:3000]))
plt.plot(n_components_range, bic)
plt.xlabel("Number of Components")
plt.xlabel("BIC")
```
:::


## Predicting and Inspecting Cluster Labels

| Code | Description |
| --- | --- |
| `labels=gmm.predict(X)` | Assign a cluster label to each observation in `X` using the fitted model `gmm` |
| `cluster_probs = gmm.predict_proba(X)` | For each observation in `X` get the probability that it belongs to any cluster in the model `gmm` |

---

:::{#exr-}
The code below fits a `GaussianMixture` model with `n_components=13` (which is the optimal number of components according to BIC) to the whole data and then predicts the `spike_labels` and the probability of cluster membership `cluster_probs`. How man unique values are there in `spike_labels` and what is the shape of `cluster_probs`?
```{python}
gmm = GaussianMixture(n_components=13,
)
gmm.fit(X)
spike_labels = gmm.predict(X)
cluster_probs = gmm.predict_proba(X)
```
:::
:::{sol.}
```{python}
print(len(np.unique(spike_labels)))
print(cluster_probs.shape)
```
:::

:::{#exr-}
What is the `spike_label` assigned to the first spike? How many spikes have been assigned to the same label?
:::
:::{sol.}
```{python}
print(spike_labels[0])
sum(spike_labels==spike_labels[0])
```
:::

:::{#exr-}
Plot the probability of cluster membership `cluster_prob` for the first spike as a histogram.
:::
:::{sol.}
```{python}
plt.hist(cluster_probs[0])
```
:::

:::{#exr-}
Plot the probability of cluster membership `cluster_prob` for spike number 656. Would you be confident in assigning this spike to a specific cluster?
:::
:::{sol.}
```{python}
plt.hist(cluster_probs[656])
```
:::

:::{#exr-}
What is the sum of all `cluster_probs` for a given spike?
:::
:::{sol.}
```{python}
sum(cluster_probs[0])
```
:::

:::{#exr-}
Get the largest cluster membership probability for each spike (i.e. `cluster_probs.max(axis=1)` and plot them in a histogram.
:::
:::{sol.}
```{python}
plt.hist(cluster_probs.max(axis=1))
```
:::

:::{#exm-}
Reproduce the graph from @exm-pca, plotting the amplitude of the 1st PCA feature at **channel 1** against the amplitude at **channel 2** for the spikes from **clusters 1, 2 and 3**.
:::
```{python}
clusters = [0, 1, 2]
for cluster in clusters:
    X = waveforms_transformed[spike_labels == cluster]
    plt.scatter(X[:, 0, 0], X[:, 0, 1])
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
```

:::{#exr-}
The code below reproduces the graph from @exm-pca, plotting the amplitude of the 1st PCA feature at **channel 1** against the amplitude at **channel 2** and highlight the spikes from **clusters 1, 2 and 3**. Complete the `for` loop below by selecting all spikes from `waveforms_transformed` that have been assigned to the given `cluster` and store them in the variable `X`.
```{python}
# | eval: false
clusters = [0, 1, 2, 3]
X = waveforms_transformed
plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, color="gray")
for cluster in clusters:
    X = 
    plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, label=cluster)
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
plt.legend()
```
:::
:::{sol.}
```{python}
clusters = [0, 1, 2, 3]
X = waveforms_transformed
plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, color="gray")
for cluster in clusters:
    X = waveforms_transformed[spike_labels == cluster]
    plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, label=cluster)
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
plt.legend()
```
:::

:::{#exr-}
Recreate the graph from the previous exercise but plot the amplitude at **channel 1** against the amplitude at the **channel 3**. Which of the clusters likely represents a distinct unit?
:::
:::{sol.}
```{python}
clusters = [0, 1, 2, 3]
X = waveforms_transformed
plt.scatter(X[:, 0, 0], X[:, 0, 1], s=1, color="gray")
for cluster in clusters:
    X = waveforms_transformed[spike_labels == cluster]
    plt.scatter(X[:, 0, 0], X[:, 0, 2], s=1, label=cluster)
    plt.xlabel("Amplitude Ch 1 [a.u.]")
    plt.ylabel("Amplitude Ch 2 [a.u.]")
plt.legend()
```
:::
